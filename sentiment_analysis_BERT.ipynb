{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "# Preliminaries\n",
    "\n",
    "from torchtext.legacy.data import Field, TabularDataset, BucketIterator, Iterator\n",
    "\n",
    "# Models\n",
    "\n",
    "import torch.nn as nn\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "\n",
    "# Training\n",
    "\n",
    "import torch.optim as optim\n",
    "\n",
    "# Evaluation\n",
    "\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_folder = 'data/sentiment_treebank'\n",
    "destination_folder = 'logs/sentiment_analysis'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(destination_folder, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Model parameter\n",
    "MAX_SEQ_LEN = 64\n",
    "PAD_INDEX = tokenizer.convert_tokens_to_ids(tokenizer.pad_token)\n",
    "UNK_INDEX = tokenizer.convert_tokens_to_ids(tokenizer.unk_token)\n",
    "\n",
    "\n",
    "\n",
    "label_field = Field(sequential=False, use_vocab=False, batch_first=True, dtype=torch.float)\n",
    "\n",
    "text_field = Field(use_vocab=False, tokenize=tokenizer.encode, lower=False, include_lengths=False, batch_first=True,\n",
    "                   fix_length=MAX_SEQ_LEN, pad_token=PAD_INDEX, unk_token=UNK_INDEX)\n",
    "fields = [('label', label_field), ('text', text_field)]\n",
    "\n",
    "# TabularDataset\n",
    "\n",
    "train, valid = TabularDataset.splits(path=source_folder, train='train.csv', validation='eval.csv',\n",
    "                                           format='CSV', fields=fields, skip_header=True)\n",
    "\n",
    "\n",
    "# Iterators\n",
    "\n",
    "train_iter = BucketIterator(train, batch_size=16, sort_key=lambda x: len(x.text),\n",
    "                            device=device, train=True, sort=True, sort_within_batch=True)\n",
    "valid_iter = BucketIterator(valid, batch_size=16, sort_key=lambda x: len(x.text),\n",
    "                            device=device, train=True, sort=True, sort_within_batch=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERT(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(BERT, self).__init__()\n",
    "\n",
    "        options_name = \"bert-base-uncased\"\n",
    "        self.encoder = BertForSequenceClassification.from_pretrained(options_name, num_labels=3)\n",
    "\n",
    "    def forward(self, text, label):\n",
    "        loss, text_fea = self.encoder(text, labels=label)[:2]\n",
    "\n",
    "        return loss, text_fea"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save and Load Functions\n",
    "\n",
    "def save_checkpoint(save_path, model, valid_loss):\n",
    "\n",
    "    if save_path == None:\n",
    "        return\n",
    "    \n",
    "    state_dict = {'model_state_dict': model.state_dict(),\n",
    "                  'valid_loss': valid_loss}\n",
    "    \n",
    "    torch.save(state_dict, save_path)\n",
    "    print(f'Model saved to ==> {save_path}')\n",
    "\n",
    "def load_checkpoint(load_path, model):\n",
    "    \n",
    "    if load_path==None:\n",
    "        return\n",
    "    \n",
    "    state_dict = torch.load(load_path, map_location=device)\n",
    "    print(f'Model loaded from <== {load_path}')\n",
    "    \n",
    "    model.load_state_dict(state_dict['model_state_dict'])\n",
    "    return state_dict['valid_loss']\n",
    "\n",
    "\n",
    "def save_metrics(save_path, train_loss_list, valid_loss_list, global_steps_list):\n",
    "\n",
    "    if save_path == None:\n",
    "        return\n",
    "    \n",
    "    state_dict = {'train_loss_list': train_loss_list,\n",
    "                  'valid_loss_list': valid_loss_list,\n",
    "                  'global_steps_list': global_steps_list}\n",
    "    \n",
    "    torch.save(state_dict, save_path)\n",
    "    print(f'Model saved to ==> {save_path}')\n",
    "\n",
    "\n",
    "def load_metrics(load_path):\n",
    "\n",
    "    if load_path==None:\n",
    "        return\n",
    "    \n",
    "    state_dict = torch.load(load_path, map_location=device)\n",
    "    print(f'Model loaded from <== {load_path}')\n",
    "    \n",
    "    return state_dict['train_loss_list'], state_dict['valid_loss_list'], state_dict['global_steps_list']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# Training Function\n",
    "\n",
    "def train_model(model,\n",
    "          optimizer,\n",
    "          criterion = nn.BCELoss(),\n",
    "          train_loader = train_iter,\n",
    "          valid_loader = valid_iter,\n",
    "          num_epochs = 5,\n",
    "          eval_every = len(train_iter),\n",
    "          file_path = destination_folder,\n",
    "          best_valid_loss = float(\"Inf\")):\n",
    "    \n",
    "    # initialize running values\n",
    "    running_loss = 0.0\n",
    "    running_step = 0\n",
    "    valid_running_loss = 0.0\n",
    "    valid_running_step = 0\n",
    "    global_step = 0\n",
    "    train_loss_list = []\n",
    "    valid_loss_list = []\n",
    "    global_steps_list = []\n",
    "\n",
    "    # training loop\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f'---> epoch {epoch+1} <---')\n",
    "        \n",
    "        \n",
    "        time.sleep(0.5)\n",
    "        loader = tqdm(train_loader, postfix={'Epoch': epoch+1})\n",
    "        model.train()\n",
    "        \n",
    "        for (labels, text), _ in loader:\n",
    "            \n",
    "            labels = labels.type(torch.LongTensor)\n",
    "            labels = labels.to(device)\n",
    "            text = text.type(torch.LongTensor)  \n",
    "            text = text.to(device)\n",
    "            output = model(text, labels)\n",
    "            loss, _ = output\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # update running values\n",
    "            running_loss += loss.item()\n",
    "            global_step += 1\n",
    "            running_step += 1\n",
    "            \n",
    "            loader.set_postfix({\n",
    "                'Epoch': epoch+1,\n",
    "                'Train loss': running_loss / running_step,\n",
    "            }, refresh=True)\n",
    "            \n",
    "            \n",
    "        # evaluation step\n",
    "        time.sleep(0.5)\n",
    "        loader = tqdm(valid_loader, postfix={'Epoch': epoch + 1,}, colour='green')\n",
    "        \n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            label_preds = []\n",
    "            # validation loop\n",
    "            for (labels, text), _ in loader:\n",
    "                labels = labels.type(torch.LongTensor)           \n",
    "                labels = labels.to(device)\n",
    "                text = text.type(torch.LongTensor)  \n",
    "                text = text.to(device)\n",
    "                output = model(text, labels)\n",
    "                loss, label_pred = output\n",
    "\n",
    "                valid_running_loss += loss.item()\n",
    "                valid_running_step += 1\n",
    "\n",
    "                label_preds.extend((torch.argmax(label_pred, dim=1) == labels).cpu().numpy() * 1)\n",
    "\n",
    "                loader.set_postfix({\n",
    "                    'Epoch': epoch+1,\n",
    "                    'Valid loss': valid_running_loss / valid_running_step,\n",
    "                    'Valid score': np.mean(label_preds)\n",
    "                }, refresh=True)\n",
    "\n",
    "        # evaluation\n",
    "        average_train_loss = running_loss / eval_every\n",
    "        average_valid_loss = valid_running_loss / len(valid_loader)\n",
    "        train_loss_list.append(average_train_loss)\n",
    "        valid_loss_list.append(average_valid_loss)\n",
    "        global_steps_list.append(global_step)\n",
    "\n",
    "        # resetting running values\n",
    "        running_loss = 0.0          \n",
    "        running_step = 0\n",
    "\n",
    "        valid_running_loss = 0.0\n",
    "        valid_running_step = 0\n",
    "\n",
    "\n",
    "\n",
    "        # print progress\n",
    "        time.sleep(0.5)\n",
    "        \n",
    "        loader.write('Epoch [{}/{}], Step [{}/{}], Train Loss: {:.4f}, Valid Loss: {:.4f}'\n",
    "              .format(epoch+1, num_epochs, global_step, num_epochs*len(train_loader),\n",
    "                      average_train_loss, average_valid_loss))\n",
    "\n",
    "        # checkpoint\n",
    "        if best_valid_loss > average_valid_loss:\n",
    "            best_valid_loss = average_valid_loss\n",
    "            save_checkpoint(file_path + '/' + 'model_bert.pt', model, best_valid_loss)\n",
    "            save_metrics(file_path + '/' + 'metrics_bert.pt', train_loss_list, valid_loss_list, global_steps_list)\n",
    "\n",
    "            loader.write('*** save ***')\n",
    "\n",
    "        time.sleep(0.5)\n",
    "    \n",
    "    save_metrics(file_path + '/' + 'metrics_bert.pt', train_loss_list, valid_loss_list, global_steps_list)\n",
    "    \n",
    "    print('Finished Training!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = BERT().to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=2e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---> epoch 1 <---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 593/593 [02:06<00:00,  4.68it/s, Epoch=1, Train loss=0.784]\n",
      "100%|\u001b[32m██████████\u001b[0m| 149/149 [00:09<00:00, 15.05it/s, Epoch=1, Valid loss=0.753, Valid score=0.73] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Step [593/2965], Train Loss: 0.7845, Valid Loss: 0.7527\n",
      "Model saved to ==> logs/sentiment_analysis/bert/model.pt\n",
      "Model saved to ==> logs/sentiment_analysis/bert/metrics.pt\n",
      "*** save ***\n",
      "---> epoch 2 <---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 593/593 [02:09<00:00,  4.58it/s, Epoch=2, Train loss=0.783]\n",
      "100%|\u001b[32m██████████\u001b[0m| 149/149 [00:10<00:00, 14.82it/s, Epoch=2, Valid loss=0.753, Valid score=0.73] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/5], Step [1186/2965], Train Loss: 0.7827, Valid Loss: 0.7526\n",
      "Model saved to ==> logs/sentiment_analysis/bert/model.pt\n",
      "Model saved to ==> logs/sentiment_analysis/bert/metrics.pt\n",
      "*** save ***\n",
      "---> epoch 3 <---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 593/593 [02:10<00:00,  4.55it/s, Epoch=3, Train loss=0.781]\n",
      "100%|\u001b[32m██████████\u001b[0m| 149/149 [00:10<00:00, 14.78it/s, Epoch=3, Valid loss=0.754, Valid score=0.73] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/5], Step [1779/2965], Train Loss: 0.7808, Valid Loss: 0.7535\n",
      "---> epoch 4 <---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 593/593 [02:12<00:00,  4.47it/s, Epoch=4, Train loss=0.783]\n",
      "100%|\u001b[32m██████████\u001b[0m| 149/149 [00:10<00:00, 14.46it/s, Epoch=4, Valid loss=0.753, Valid score=0.73] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/5], Step [2372/2965], Train Loss: 0.7828, Valid Loss: 0.7531\n",
      "---> epoch 5 <---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 593/593 [02:14<00:00,  4.42it/s, Epoch=5, Train loss=0.781]\n",
      "100%|\u001b[32m██████████\u001b[0m| 149/149 [00:10<00:00, 14.49it/s, Epoch=5, Valid loss=0.753, Valid score=0.73] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/5], Step [2965/2965], Train Loss: 0.7809, Valid Loss: 0.7532\n",
      "Model saved to ==> logs/sentiment_analysis/bert/metrics.pt\n",
      "Finished Training!\n"
     ]
    }
   ],
   "source": [
    "train_model(model=model, optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0], device='cuda:0') tensor([0], device='cuda:0')\n",
      "tensor([0], device='cuda:0') tensor([0], device='cuda:0')\n",
      "tensor([0], device='cuda:0') tensor([0], device='cuda:0')\n",
      "tensor([0], device='cuda:0') tensor([0], device='cuda:0')\n",
      "tensor([0], device='cuda:0') tensor([1], device='cuda:0')\n",
      "tensor([0], device='cuda:0') tensor([0], device='cuda:0')\n",
      "tensor([0], device='cuda:0') tensor([0], device='cuda:0')\n",
      "tensor([0], device='cuda:0') tensor([1], device='cuda:0')\n",
      "tensor([0], device='cuda:0') tensor([0], device='cuda:0')\n",
      "tensor([0], device='cuda:0') tensor([0], device='cuda:0')\n",
      "tensor([0], device='cuda:0') tensor([0], device='cuda:0')\n",
      "tensor([0], device='cuda:0') tensor([0], device='cuda:0')\n",
      "tensor([0], device='cuda:0') tensor([1], device='cuda:0')\n",
      "tensor([0], device='cuda:0') tensor([0], device='cuda:0')\n",
      "tensor([0], device='cuda:0') tensor([0], device='cuda:0')\n",
      "tensor([0], device='cuda:0') tensor([0], device='cuda:0')\n",
      "tensor([0], device='cuda:0') tensor([0], device='cuda:0')\n",
      "tensor([0], device='cuda:0') tensor([0], device='cuda:0')\n",
      "tensor([0], device='cuda:0') tensor([1], device='cuda:0')\n",
      "tensor([0], device='cuda:0') tensor([0], device='cuda:0')\n",
      "tensor([0], device='cuda:0') tensor([0], device='cuda:0')\n",
      "tensor([0], device='cuda:0') tensor([0], device='cuda:0')\n",
      "tensor([0], device='cuda:0') tensor([0], device='cuda:0')\n",
      "tensor([0], device='cuda:0') tensor([0], device='cuda:0')\n",
      "tensor([0], device='cuda:0') tensor([0], device='cuda:0')\n",
      "tensor([0], device='cuda:0') tensor([0], device='cuda:0')\n",
      "tensor([0], device='cuda:0') tensor([1], device='cuda:0')\n",
      "tensor([0], device='cuda:0') tensor([0], device='cuda:0')\n",
      "tensor([0], device='cuda:0') tensor([0], device='cuda:0')\n",
      "tensor([0], device='cuda:0') tensor([2], device='cuda:0')\n",
      "tensor([0], device='cuda:0') tensor([0], device='cuda:0')\n",
      "tensor([0], device='cuda:0') tensor([0], device='cuda:0')\n",
      "tensor([0], device='cuda:0') tensor([0], device='cuda:0')\n",
      "tensor([0], device='cuda:0') tensor([2], device='cuda:0')\n",
      "tensor([0], device='cuda:0') tensor([1], device='cuda:0')\n",
      "tensor([0], device='cuda:0') tensor([0], device='cuda:0')\n",
      "tensor([0], device='cuda:0') tensor([0], device='cuda:0')\n",
      "tensor([0], device='cuda:0') tensor([2], device='cuda:0')\n",
      "tensor([0], device='cuda:0') tensor([0], device='cuda:0')\n",
      "tensor([0], device='cuda:0') tensor([0], device='cuda:0')\n",
      "tensor([0], device='cuda:0') tensor([0], device='cuda:0')\n",
      "tensor([0], device='cuda:0') tensor([1], device='cuda:0')\n",
      "tensor([0], device='cuda:0') tensor([0], device='cuda:0')\n",
      "tensor([0], device='cuda:0') tensor([0], device='cuda:0')\n",
      "tensor([0], device='cuda:0') tensor([0], device='cuda:0')\n",
      "tensor([0], device='cuda:0') tensor([0], device='cuda:0')\n",
      "tensor([0], device='cuda:0') tensor([0], device='cuda:0')\n",
      "tensor([0], device='cuda:0') tensor([0], device='cuda:0')\n",
      "tensor([0], device='cuda:0') tensor([0], device='cuda:0')\n",
      "tensor([0], device='cuda:0') tensor([0], device='cuda:0')\n",
      "tensor([0], device='cuda:0') tensor([0], device='cuda:0')\n",
      "tensor([0], device='cuda:0') tensor([0], device='cuda:0')\n",
      "tensor([0], device='cuda:0') tensor([2], device='cuda:0')\n",
      "tensor([0], device='cuda:0') tensor([0], device='cuda:0')\n",
      "tensor([0], device='cuda:0') tensor([0], device='cuda:0')\n",
      "tensor([0], device='cuda:0') tensor([1], device='cuda:0')\n",
      "tensor([0], device='cuda:0') tensor([2], device='cuda:0')\n",
      "tensor([0], device='cuda:0') tensor([0], device='cuda:0')\n",
      "tensor([0], device='cuda:0') tensor([0], device='cuda:0')\n",
      "tensor([0], device='cuda:0') tensor([0], device='cuda:0')\n",
      "tensor([0], device='cuda:0') tensor([0], device='cuda:0')\n",
      "tensor([0], device='cuda:0') tensor([0], device='cuda:0')\n",
      "tensor([0], device='cuda:0') tensor([0], device='cuda:0')\n",
      "tensor([0], device='cuda:0') tensor([0], device='cuda:0')\n",
      "tensor([0], device='cuda:0') tensor([1], device='cuda:0')\n",
      "tensor([0], device='cuda:0') tensor([1], device='cuda:0')\n",
      "tensor([0], device='cuda:0') tensor([0], device='cuda:0')\n",
      "tensor([0], device='cuda:0') tensor([0], device='cuda:0')\n",
      "tensor([0], device='cuda:0') tensor([2], device='cuda:0')\n",
      "tensor([0], device='cuda:0') tensor([0], device='cuda:0')\n",
      "tensor([0], device='cuda:0') tensor([0], device='cuda:0')\n",
      "tensor([0], device='cuda:0') tensor([0], device='cuda:0')\n",
      "tensor([0], device='cuda:0') tensor([0], device='cuda:0')\n",
      "tensor([0], device='cuda:0') tensor([0], device='cuda:0')\n",
      "tensor([0], device='cuda:0') tensor([0], device='cuda:0')\n",
      "tensor([0], device='cuda:0') tensor([0], device='cuda:0')\n",
      "tensor([0], device='cuda:0') tensor([0], device='cuda:0')\n",
      "tensor([0], device='cuda:0') tensor([0], device='cuda:0')\n",
      "tensor([0], device='cuda:0') tensor([0], device='cuda:0')\n",
      "tensor([0], device='cuda:0') tensor([0], device='cuda:0')\n",
      "tensor([0], device='cuda:0') tensor([0], device='cuda:0')\n",
      "tensor([0], device='cuda:0') tensor([0], device='cuda:0')\n",
      "tensor([0], device='cuda:0') tensor([0], device='cuda:0')\n",
      "tensor([0], device='cuda:0') tensor([0], device='cuda:0')\n",
      "tensor([0], device='cuda:0') tensor([0], device='cuda:0')\n",
      "tensor([0], device='cuda:0') tensor([0], device='cuda:0')\n",
      "tensor([0], device='cuda:0') tensor([1], device='cuda:0')\n",
      "tensor([0], device='cuda:0') tensor([1], device='cuda:0')\n",
      "tensor([0], device='cuda:0') tensor([0], device='cuda:0')\n",
      "tensor([0], device='cuda:0') tensor([0], device='cuda:0')\n",
      "tensor([0], device='cuda:0') tensor([0], device='cuda:0')\n",
      "tensor([0], device='cuda:0') tensor([1], device='cuda:0')\n",
      "tensor([0], device='cuda:0') tensor([1], device='cuda:0')\n",
      "tensor([0], device='cuda:0') tensor([0], device='cuda:0')\n",
      "tensor([0], device='cuda:0') tensor([0], device='cuda:0')\n",
      "tensor([0], device='cuda:0') tensor([0], device='cuda:0')\n",
      "tensor([0], device='cuda:0') tensor([0], device='cuda:0')\n",
      "tensor([0], device='cuda:0') tensor([1], device='cuda:0')\n",
      "tensor([0], device='cuda:0') tensor([1], device='cuda:0')\n",
      "tensor([0], device='cuda:0') tensor([0], device='cuda:0')\n",
      "tensor([0], device='cuda:0') tensor([1], device='cuda:0')\n",
      "tensor([0], device='cuda:0') tensor([1], device='cuda:0')\n",
      "tensor([0], device='cuda:0') tensor([0], device='cuda:0')\n",
      "tensor([0], device='cuda:0') tensor([0], device='cuda:0')\n",
      "tensor([0], device='cuda:0') tensor([1], device='cuda:0')\n",
      "tensor([0], device='cuda:0') tensor([0], device='cuda:0')\n",
      "tensor([0], device='cuda:0') tensor([0], device='cuda:0')\n",
      "tensor([0], device='cuda:0') tensor([0], device='cuda:0')\n",
      "tensor([0], device='cuda:0') tensor([1], device='cuda:0')\n",
      "tensor([0], device='cuda:0') tensor([0], device='cuda:0')\n",
      "tensor([0], device='cuda:0') tensor([0], device='cuda:0')\n",
      "tensor([0], device='cuda:0') tensor([0], device='cuda:0')\n",
      "tensor([0], device='cuda:0') tensor([0], device='cuda:0')\n",
      "tensor([0], device='cuda:0') tensor([1], device='cuda:0')\n",
      "tensor([0], device='cuda:0') tensor([0], device='cuda:0')\n",
      "tensor([0], device='cuda:0') tensor([0], device='cuda:0')\n",
      "tensor([0], device='cuda:0') tensor([0], device='cuda:0')\n",
      "tensor([0], device='cuda:0') tensor([0], device='cuda:0')\n",
      "tensor([0], device='cuda:0') tensor([0], device='cuda:0')\n",
      "tensor([0], device='cuda:0') tensor([0], device='cuda:0')\n",
      "tensor([0], device='cuda:0') tensor([0], device='cuda:0')\n",
      "tensor([0], device='cuda:0') tensor([0], device='cuda:0')\n",
      "tensor([0], device='cuda:0') tensor([1], device='cuda:0')\n",
      "tensor([0], device='cuda:0') tensor([0], device='cuda:0')\n",
      "tensor([0], device='cuda:0') tensor([0], device='cuda:0')\n",
      "tensor([0], device='cuda:0') tensor([0], device='cuda:0')\n",
      "tensor([0], device='cuda:0') tensor([1], device='cuda:0')\n",
      "tensor([0], device='cuda:0') tensor([0], device='cuda:0')\n",
      "tensor([0], device='cuda:0') tensor([1], device='cuda:0')\n",
      "tensor([0], device='cuda:0') tensor([1], device='cuda:0')\n",
      "tensor([0], device='cuda:0') tensor([1], device='cuda:0')\n",
      "tensor([0], device='cuda:0') tensor([0], device='cuda:0')\n",
      "tensor([0], device='cuda:0') tensor([0], device='cuda:0')\n",
      "tensor([0], device='cuda:0') tensor([0], device='cuda:0')\n",
      "tensor([0], device='cuda:0') tensor([2], device='cuda:0')\n",
      "tensor([0], device='cuda:0') tensor([0], device='cuda:0')\n",
      "tensor([0], device='cuda:0') tensor([0], device='cuda:0')\n",
      "tensor([0], device='cuda:0') tensor([0], device='cuda:0')\n",
      "tensor([0], device='cuda:0') tensor([0], device='cuda:0')\n",
      "tensor([0], device='cuda:0') tensor([2], device='cuda:0')\n",
      "tensor([0], device='cuda:0') tensor([0], device='cuda:0')\n",
      "tensor([0], device='cuda:0') tensor([0], device='cuda:0')\n",
      "tensor([0], device='cuda:0') tensor([0], device='cuda:0')\n",
      "tensor([0], device='cuda:0') tensor([0], device='cuda:0')\n",
      "tensor([0], device='cuda:0') tensor([0], device='cuda:0')\n",
      "tensor([0], device='cuda:0') tensor([0], device='cuda:0')\n",
      "tensor([0], device='cuda:0') tensor([2], device='cuda:0')\n",
      "tensor([0], device='cuda:0') tensor([0], device='cuda:0')\n",
      "tensor([0], device='cuda:0') tensor([0], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "for (labels, text), _ in valid_iter:\n",
    "    labels = labels.type(torch.LongTensor)           \n",
    "    labels = labels[:1] .to(device)\n",
    "    text = text.type(torch.LongTensor)  \n",
    "    text = text[:1].to(device)\n",
    "    \n",
    "    _, output = model(text, labels)\n",
    "    print(output.argmax(dim=1), labels)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
